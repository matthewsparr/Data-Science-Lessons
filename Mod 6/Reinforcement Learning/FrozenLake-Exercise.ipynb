{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Frozen Lake\n",
    "### <center>Using Q-learning to not die"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "S F F F <br>\n",
    "F H F H <br>\n",
    "F F F H <br>\n",
    "H F F G <br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S = starting position<br>\n",
    "F = frozen, or safe<br>\n",
    "H = hole, or death<br>\n",
    "G = goal<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='frozenlake.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import all the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's watch some random playing of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\", is_slippery=False)\n",
    "for game in range(10):\n",
    "    state = env.reset()\n",
    "    for t in range(100):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.2)\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Discuss\n",
    "#### <center> What do you observe from watching the random agent? \n",
    "#### <center> Do you think this will be an easy environment to solve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try using Q-learning to train an agent to play the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a version of FrozenLake where the surface is not slippery. This is a deterministic environment -  every action we choose to take will always move us in that direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need the following elements:\n",
    "- <b>Q-table</b>:\n",
    "    - Instantiate a table of zeros of size (number of states)x(number of actions)\n",
    "    - You can use np.zeros() to create the array.\n",
    "    \n",
    "- <b>Gamma</b>:\n",
    "    - The discount factor for future rewards.\n",
    "    - Typically a number close to 1 (0.95-0.99) but feel free to try different values.\n",
    "- <b>Scores array</b>:\n",
    "    - An empty list to append scores from each game to.\n",
    "    - Also, set a running score variable to zero at the start of each game.\n",
    "- <b>Action selection</b>:\n",
    "    - At each step, choose the best action.\n",
    "    - Since at the beginning, our table will be all zeros, we need to be able to choose an action at random if more than one has the highest Q-value.\n",
    "    - You can use np.max() to get the highest Q-value.\n",
    "    - Then get the action for each of those highest Q-values with np.where(). (Note: np.where() returns a tuple where the first element is the array of actions.)\n",
    "    - Then use np.random.choice() to select one of the best actions.\n",
    "- <b>Q-value calculation</b>:\n",
    "    - Using the Q-formula:\n",
    "    <img src='q_formula.png'>\n",
    "    - You will have to find the maximum Q-value at the next state.\n",
    "    - You will want to set the appropriate value in your Q-table to this.\n",
    "- <b>Reward tracking</b>:\n",
    "    - Add the current reward to the running score total\n",
    "    - After each game, store the game score in the scores list\n",
    "- <b>State transition</b>\n",
    "    - Set the next state as the current state.\n",
    "- <b>Termination</b>\n",
    "    - If the agent managed to not fall in a hole and thus made it to the goal for 10 episodes in a row, then the environment is considered solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthew\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Matthew\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\", is_slippery=False)\n",
    "\n",
    "Q_table = None\n",
    "gamma = None\n",
    "scores = []\n",
    "\n",
    "## We will play for a maximum of 10000 games.\n",
    "for episode in range(10000):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    while not done:\n",
    "        ## Choose action\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        ## Calculate Q-value\n",
    "        \n",
    "        ## Set appropriate value in Q-table to Q-value\n",
    "        \n",
    "        ## Add current reward to running score total\n",
    "        \n",
    "        ## Set the current state to the next_state\n",
    "        \n",
    "    ## Add the score for this game to the scores list\n",
    "    \n",
    "    \n",
    "    ## If we have played at least 10 games, check if the last 10 games resulted in success\n",
    "    if episode>9:\n",
    "        if np.mean(scores[-10:])==1:\n",
    "            print(\"Solved!\")\n",
    "            print(\"Average per past 100 games: \" + str(np.mean(scores[-100:])))\n",
    "            print(\"Number of games played: \" + str(episode))\n",
    "            break\n",
    "            \n",
    "if np.mean(scores[-10:])!=1:\n",
    "    print('Failed')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at your Q-table now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-77cc7e71fc2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mq_table_not_slippery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_table\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mq_table_not_slippery\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mround_\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mround_\u001b[1;34m(a, decimals, out)\u001b[0m\n\u001b[0;32m   3597\u001b[0m     \u001b[0maround\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mequivalent\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0msee\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3598\u001b[0m     \"\"\"\n\u001b[1;32m-> 3599\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36maround\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36maround\u001b[1;34m(a, decimals, out)\u001b[0m\n\u001b[0;32m   3222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3223\u001b[0m     \"\"\"\n\u001b[1;32m-> 3224\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'round'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "q_table_not_slippery = np.round(Q_table,3)\n",
    "q_table_not_slippery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Discuss\n",
    "#### <center> How did your agent do? \n",
    "#### <center> What does your Q-table look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to try a stochastic version of our environment by setting is_slippery to true. There is a now a chance that we will slide on the ice and move a different direction than what we chose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Discuss\n",
    "#### <center> How do you think the challenge will change in the new, slippery version of the environment?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your code from above and paste below with the following changes:\n",
    "- <b>is_slippery</b>:\n",
    "    - Set to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for slippery environment agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at your Q-table now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table_is_slippery = np.round(Q_table,3)\n",
    "q_table_is_slippery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Discuss\n",
    "#### <center> How did your agent perform in the slippery environment?\n",
    "#### <center> Why is performance different in this environment?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Greedy epsilon policy\n",
    "<center>Makes the agent explore the environment more at first and then later on relying only on its knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Learning rate\n",
    "<center>Allows the agent to accumulate knowledge, not just overriding previous Q-values with each new experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Q[s, a] = Q[s, a] + alpha*(R + gamma*Max[Q(sâ€™, A)] - Q[s, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\", is_slippery=True)\n",
    "\n",
    "Q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "gamma = 0.95\n",
    "scores = []\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "alpha = 0.85\n",
    "\n",
    "for episode in range(10000):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.random.choice(np.where(Q_table[state,:]==np.max(Q_table[state,:]))[0])\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        Q_value = reward + gamma * np.max(Q_table[next_state, :])\n",
    "        Q_table[state, action] = Q_table[state, action] + alpha*(Q_value-Q_table[state,action])\n",
    "        score += reward\n",
    "        state = next_state     \n",
    "    scores.append(score)\n",
    "    epsilon = epsilon*epsilon_decay\n",
    "    if episode>99:\n",
    "        if np.mean(scores[-10:])==1:\n",
    "            print(\"Solved!\")\n",
    "            print(\"Average per past 10 games: \" + str(np.mean(scores[-10:])))\n",
    "            print(\"Number of games played: \" + str(episode))\n",
    "            break\n",
    "     \n",
    "    if episode%100==0:\n",
    "        print('Average success rate: ', np.mean(scores), ' Epsilon: ', epsilon)\n",
    "if np.mean(scores[-10:])!=1:\n",
    "    print('Failed')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table_is_slippery_solved = np.round(Q_table,3)\n",
    "q_table_is_slippery_solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
